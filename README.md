# Recurrent Neural Networks (RNN)

## Overview
This project implements a **Recurrent Neural Network (RNN)** for sequence-based tasks using PyTorch. It includes scripts for model training and evaluation.


# Modular Neural Network Framework


A lightweight, modular implementation of neural network components from scratch, including **RNNs, LSTMs, and traditional layers** (convolutional, pooling, dropout, etc.). Designed for educational purposes and custom deep learning projects.

## Project Structure

├── Base.py # Base classes for layers/networks
├── BatchNormalization.py # Batch normalization layer
├── Constraints.py # Weight constraints
├── Conv.py # Convolutional layer
├── Dropout.py # Dropout layer
├── Flatten.py # Flatten layer
├── FullyConnected.py # Dense/fully connected layer
├── Initializers.py # Weight initialization (Glorot, He, etc.)
├── LSTM.py # LSTM layer implementation
├── Loss.py # Loss functions (MSE, CrossEntropy)
├── NeuralNetwork.py # Main neural network class
├── Optimizers.py # Optimizers (SGD, Adam, RMSprop)
├── Pooling.py # Pooling layers (Max, Average)
├── RNN.py # Vanilla RNN layer
├── ReLU.py # ReLU activation
├── Sigmoid.py # Sigmoid activation
├── SoftMax.py # Softmax activation
├── TanH.py # Tanh activation
